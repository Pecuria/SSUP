目标：在这里，我们研究灵活使用工具的认知和计算基础。虽然人类工具的使用依赖于许多认知系统，例如，知道如何掌握和操纵物体或理解特定工具的典型使用方式，但在这里我们关注的是“机械推理”，即自发地重新调整环境中物体用途以实现新目标的能力

有人认为，机械推理依赖于心理模拟，这使我们能够预测我们的行为将如何导致世界的变化（3）。这种通用模拟是支持我们在新环境中推理对象的能力的必要组成部分，但它本身并不能解释我们如何如此快速地制定和更新我们的计划。我们建议，快速使用工具的另一个关键是知道应该考虑什么样的行动——从最初对什么行动有用的理解，到通过观察我们在模拟和现实中的行动结果来更新这一信念。

本文有两个贡献。首先，我们介绍虚拟工具游戏，它提供了一套解决物理问题的挑战，并允许在人类和机器代理之间进行精确、可量化的比较。其次，我们提出了一种灵活工具使用的最小模型，称为“采样、模拟、更新”（SSUP）。该模型是围绕一个高效但嘈杂的模拟引擎构建的，该引擎允许模型在各种物理任务中灵活行动。为了快速解决问题，SSUP模型以可能解决问题的候选工具和行动的结构化先验形式包含了关于世界的丰富知识，这使其能够将模拟限制在有前景的候选对象上。它进一步从模拟和观察自身行动的结果中学习，以更新其对这些有前途的候选者应该是什么的信念。在两个实验的30个虚拟工具级别上，我们表明，SSUP模型的实例化捕捉了人类玩家不同级别的相对困难、为解决每个级别而执行的特定行动，以及每个级别的解决率如何演变。

The Virtual Tools Game:

这个游戏要求玩家将几个物体（“工具”）中的一个放入二维（2D）动态物理环境中，以实现一个目标：将红色物体放入绿色区域（图1D）。每个级别的目标都是一样的，但实现它所需的条件差异很大。一旦放置了一个工具，世界的物理就被启用了，这样玩家就可以看到他们所采取的行动的效果。如果目标没有实现，玩家可以将世界“重置”为原始状态并重试；每次尝试时，它们仅限于一个动作。我们设计了30个级别——20个用于原始实验（图2），10个用于验证实验（见图7A）——来测试“发射”、“阻断”和“支持”等概念。在前20个级别中，12个级别由六个“配对（matched pairs）”组成，这些配对包含了场景中目标或物体的微小差异，以测试刺激的细微差异是否会导致可观察到的行为差异。

虚拟工具游戏提出了特殊的挑战，我们认为这些挑战是更普遍地快速解决物理问题所需的推理的基础。首先，有各种各样的任务需要不同的策略和物理概念来解决，但需要使用接近现实世界的共享物理动力学。其次，游戏需要长期因果推理。由于玩家只能通过放置一个物体与游戏互动，因此他们必须能够在未来很长一段时间内，当他们无法再干预时，对其行为的复杂因果关系进行推理。最后，该游戏在人类中引发了快速的试错学习。人类玩家通常不会在第一次尝试时解决关卡，但通常也不需要超过5到10次尝试才能成功。人们表现出各种各样的解决问题的行为，包括突然发现如何解决特定任务的正确想法的“啊哈”见解，以及渐进的试错策略改进。图3展示了这在实践中是如何发生的，显示了四个不同的例子，参与者快速或缓慢地学习，并发现了在不同层次上使用工具的不同方法。

SSUP algorithm:

我们考虑了捕捉人类工具使用的灵活性和效率所需的组件。我们建议人们通过一种内部心理模型来实现灵活性，这种模型允许他们想象他们可能从未尝试过的行为的效果（“模拟”）。然而，仅靠心理模型是不够的——有太多可能的行为可以模拟，其中许多行为缺乏信息，不太可能实现特定的目标。为了专注于假设空间的有用部分，需要一些指导内部搜索的机制。因此，我们建议人们使用结构化的、面向对象的先验（“样本”）和快速信念更新机制（“更新”）来指导对有希望的假设的搜索。我们在SSUP模型中用这些组件将人工工具的使用形式化（图4A）

SSUP的灵感来自“问题解决即搜索（problem solving as  search）”理论（12），以及Dyna和其他基于模型的策略优化方法（13，14）。至关重要的是，我们假设结构化先验和物理模拟器必须已经到位，才能像人类一样快速地解决问题；因此，与大多数基于模型的策略优化方法不同，我们不执行动态模型的在线更新

我们强调，我们将SSUP视为物理问题解决的通用建模框架，并在这里仅介绍该框架的一个实例：我们认为在虚拟工具游戏中捕捉基本人类行为所需的最小模型（如下所述，更多细节见SI附录，第S2节）。在讨论中，我们强调了模型在未来工作中需要改进的方式，以及依赖于更丰富的认知系统的物理推理方面，这些认知系统超出了这里提出的框架。

Sample: Object-Base Prior:

至少，我们应该考虑实现任何目标的行动都有可能影响我们的环境。因此，我们为采样操作引入了基于对象的先验。具体来说，该模型选择场景中的一个可移动对象，然后在稍微超出对象宽度的区域中选择一个x坐标，在该对象上方或下方选择一个y坐标（图4B：示例）。对于工具选择，我们假设参与者同样有可能选择三种工具中的任何一种，因为游戏中的所有工具都是为参与者不熟悉而设计的。此发行版中的样本用于初始化搜索。

Simulate:  A Noisy Physics Engine

为了确定世界上哪些采样动作值得尝试，我们假设人们使用“直观的物理引擎”（15）来灵活地想象他们动作的效果。该引擎能够以近似正确但随机的动力学实时模拟世界（16,17）。因此，确定拟议行动的效果涉及将该行动应用于一个人的心理表征，并使用直观的物理引擎来假设该行动可能导致世界展开的一系列方式（18，19）。在这里，我们使用具有噪声动力学的游戏物理引擎来实现模拟。人们通常对碰撞将如何解决有噪声的预测（16），因此为了简单起见，我们假设结果的不确定性仅由这些碰撞中的噪声（施加在两个碰撞物体之间的力的方向和大小）驱动。*

由于内部模型不完美，为了评估一个行动，我们产生了少量的随机模拟（nsims，此处设置为4），以形成一组关于结果的假设。为了正式确定结果有多好（给定行动的奖励），我们从因果推理文献中借用了一个关于人们如何概念化“几乎”成功的想法（20）。几乎成功与否并不取决于一个行动让你朝着目标前进的绝对距离，而是取决于这个行动带来了多大的不同。为了捕捉到这一点，记录了绿色球门区域和任何红色球门物体之间的最小距离；这些值在模拟中取平均值，并按照如果没有添加工具的情况下所达到的最小距离进行归一化。SSUP中使用的奖励是1减去归一化距离，因此更近的物体会带来更高的奖励

一旦模型发现一个足够好的行动（形式化为平均奖励高于某个阈值），它就会“在世界上”采取该行动。此外，为了模拟思考的时间限制，如果模型在不采取行动的情况下考虑了超过T个不同的行动建议（此处设置为5个），它将采取迄今为止想象中的最佳行动。我们在敏感性分析中评估所有参数选择的效果

Update: Learning from Thoughts and Actions

到目前为止，我们已经描述了一种智能初始化搜索的方法，以避免考虑无用的操作。但是，如果先验仍然存在一个难以处理的巨大可能行动空间呢？

为了解决这个问题，我们引入了一种更新机制，该机制从模拟和真实经验中学习，以指导未来对假设空间中更有前景的区域的搜索（21）。这被正式定义为三种工具及其位置的高斯混合模型策略，π0（s）表示模型对每种工具的高价值行为的信念。π0（s）用面向对象先验的样本初始化，并使用简单的策略梯度算法进行更新（22）。该算法将塑造区域周围的后验信念，以放置每个工具，这些工具预计会将目标对象移动到目标附近，因此可能包含一个解决方案。当它发现邻近成功操作的高价值操作时，这种更新策略很有用，但也可能陷入不存在成功操作的局部最优值。因此，我们使用强化学习中的一种标准技术：epsilon贪婪探索。在epsilon贪婪探索中，潜在行为在100-epsilon%的时间里从策略中采样，在epsilon%的时间里从前一个策略中采样。请注意，此探索仅用于提出内部模拟；基于仿真结果集选择模型动作。这类似于思考新的东西，而不是专注于现有的战略。

Results:

我们分析了人类在虚拟工具游戏前20个级别上的表现，并将人类与SSUP模型和替代模型进行了比较，包括具有消融和两个替代学习基线的SSUP模型。我们证明，完整的SSUP模型最能捕捉到人类的表现。访问游戏和所有数据，包括人类和模型放置，可在https://k-r-allen.github.io/tool-games/

Human Results:

我们通过Amazon Mechanical Turk招募了94名参与者，并要求每位参与者解决14个级别：所有8个不匹配的级别和6对匹配对中的每一对的一个变体（随机选择）。

参与者可以选择在问题解决后或2分钟后继续。更多详细信息请参见SI附录第S1节

游戏级别之间的难度差异很大。参与者的平均解决率为81%（SD=19%），范围从最难的31%到最简单的100%。同样，参与者在每个级别平均采取了4.5个行动（SD=2.5），平均尝试次数在1.5到9.4次之间。即使在试验中，参与者用于解决水平问题的行动数量也存在很大的异质性。这可以通过“快速试错”学习来实现：最初尝试有希望的行动的参与者会很快解决这个难题，而其他人在采取有希望的动作之前会探索不同的动作（如图3）

所有六个匹配的水平对的行为都不同。我们研究这些细微的差异是否确实会影响行为，即使没有对第一个动作的反馈，通过询问我们是否可以确定每个动作来自哪个级别的变体。我们发现，在第一次尝试时，这些动作在“轴”shafts、“预防”prevention、“发射”launch和“桌子”table中的匹配级别上是可区分的，但在“坠落”falling或“塔”towars中是不可区分的（详见SI附录图S11和S6A节）。然而，参与者需要不同数量的动作来解决每个级别（所有ts>2:7，ps<0:01）。这表明人们在选择行动时会注意到场景或目标的细微差异

Model Results:

我们研究了将模型与人类数据进行比较的几个指标。首先，我们看看每个模型解决每个级别的速度和频率，以及是否与参与者匹配。这被衡量为每个级别的参与者尝试平均次数与每个级别的模型尝试平均次数之间的相关性和均方根误差（RMSE），以及人类和模型解决方案之间的相关性与RMSE。SSUP模型很好地解释了不同层次的人类行为模式（SI附录，表S2）。它在每个级别上使用了类似的尝试次数（r=0.71；95%CI=[0:62,0:76]；所有级别的平均经验尝试次数为4.48；平均模型尝试次数为4.24；图5A），并达到了类似的准确性（r=0.86；95%CI=[0:76,0:89]；图5B）。

在许多层面上，SSUP模型不仅实现了与人相同的整体解决方案率，而且以相同的速度接近它。我们通过观察累积解决方案率来衡量这一点——在所有参与者或模型运行中，在X位置内解决每个级别的比例是多少——发现人们和模型通常表现出相似的解决方案（图6A；定量比较见SI附录S6B节）。

我们可以通过比较人们和模型采取的第一个行动以及双方为解决一个级别而采取的行动，更详细地了解模型是如何实现这一点的（图6B）。与我们的人类参与者一样，该模型在匹配水平对之间的第一次尝试中采取了明显不同的行动（SI附录，第S6A节）。更一般地说，人和模型通常都会从各种看似合理的动作开始（例如弹射）。在某些情况下，双方都会尝试对现场影响很小的初始行动[例如，SeeSaw和Prevention（B）]；这可能是因为人们无法想到任何有用的操作，因此决定尝试一些东西，类似于模型如何超过其模拟阈值。然而，在其他情况下，模型的初始预测与人们不同，这导致了不同的搜索和解决方案模式。例如，在Falling（A）中，模型很快发现，将物体放在容器下会可靠地将球倒在地上，但人们倾向于从上方掉落物体。因此，模型通常会快速求解下面有对象的级别，而一部分参与者会找到从上面翻转容器的方法；这种差异也可以在解决方案之前的尝试次数的比较中看到，模型很快找到解决方案，而人们需要更长的时间（图5A）。关于所有级别的第一个和最后一个动作的比较，请参见SI附录，图S11。

Model Comparisons on Virtual Tools:

我们将完整的SSUP模型与一组六个替代模型进行了比较。三个模型通过分别删除先验、模拟或更新来研究每个SSUP组件的贡献。两种模型提出了替代解决方案：学习更好的世界模型，而不是学习动作（参数调整），或者用学习到的建议机制（深度Q网络[DQN，参考23]+更新）替换先前的和模拟器。参数调整替代模型使用推理从观察到的轨迹中学习物体密度、摩擦和弹性。学习建议机制对应于无模型的深度强化学习代理（23），该代理在4500个随机生成的游戏级别（SI附录，S5节）上进行训练，然后使用与SSUP相同的机制对20个测试级别中的每一个进行在线更新。该模型在环境方面比其他模型有更多的经验，并且可以测试无模型方法是否可以利用这种经验来学习可以指导快速学习的通用策略。最后，如果代理只是随机放置工具，我们将其与性能的“猜测”基线进行比较。这些比较见图5C和SI附录表S2。

消除三个SSUP组件中的任何一个都会导致性能显著降低（以经验和模型累积解曲线之间的偏差来衡量；所有自举ps<0:0001；有关更多详细信息，请参阅SI附录第S6B节和图S6）。简化模型通常需要更多的尝试来解决级别问题，因为它们在动作空间的错误区域进行搜索（没有先验），尝试没有成功机会的动作（没有模拟），或者不引导搜索到更有希望的区域（没有更新）。

DQN+更新在所有合理的替代模型中表现最差，使用最多的操作和解决级别，其速度几乎不超过概率。由于这相当于具有不同先验的无仿真模型，其较差的性能表明，通过反复玩类似的关卡，很难学习到通用的动作策略（SI附录，第S5节）

因为参数调整模型相当于无更新模型，除了动态模型的属性可以在参数调整中学习，比较这两个模型可以让我们测试我们是否需要假设人们在这个游戏中学习世界的动态。两个模型的性能大致相等（图5C）这一事实表明，我们在这里不需要这个假设

最后，我们量化了每个模型对人们采取的特定行动的捕捉程度。由于参与者反应的异质性，我们无法清楚地区分模型的表现，只能发现DQN+更新模型的表现不如其他模型（SI附录，S6C节）。然而，没有模型达到理论噪声上限，这表明SSUP框架的组成部分可以改进，以更好地解释参与者的行为（讨论）。

Validation on Novel Levels

我们进行了第二个实验，以测试模型是否在不调整超参数的情况下推广到新的水平和物理概念。在这个实验中，我们创建了10个新的水平：6种新的水平类型和4种原始水平的变体（图7A），在所有水平上测试了50名参与者的独立样本。这6个新的关卡类型旨在测试新的物理策略，包括平衡、打破和从球的路径上移除物体。所有其他实验细节与主实验相同

在不调整任何模型参数的情况下，我们发现人类和模型求解率之间存在良好的对应关系（图7B），模型的性能与人类在放置数量（图7C，r=0.85）和精度（图7D，r=0.95）方面的性能之间存在很强的相关性。与主要实验类似，我们发现，如果删除先前或模拟，或者对于DQN+更新模型（所有自举ps<0:0001；SI附录，图S7），性能会下降。然而，尽管在数值上更差，但如果删除更新机制（p=0:055）或将其替换为模型学习（p=0.346），我们没有发现可靠的差异，这表明特定的奖励函数或更新过程可能不太适用于这些级别（SI附录，S6B节）。

Discussion

我们介绍了用于研究人类和机器灵活解决物理问题的虚拟工具游戏，并表明人类在这一挑战中的行为表达了各种试错解决问题的策略。我们还介绍了一种解决人体物理问题的模型：采样、模拟、更新。该模型假定，为了解决这些物理问题，人们依赖于世界如何运作的内部模型。因此，在这个游戏中学习需要浓缩这些庞大的世界知识，使用结构化的试错搜索快速学习如何在每个情况下采取行动。

Model Limitations:

尽管我们使用的SSUP模型以类似人类的方式解决了虚拟工具游戏的许多级别，但我们认为这仍然只是人们为任务带来的丰富认知过程的第一个近似值。特别是，该模型至少在两个方面存在不足：一是依赖非常简单的先验，二是仅在 forward direction 进行规划和推广

我们可以在Falling（A）级别中看到基于对象的先验的局限性（图5B）：人们不太可能考虑将物体放在容器下方使其翻倒。相反，许多人试图从上面翻倒它，尽管这更难。通过这种方式，人们对策略的偏好是特定于上下文的，这导致他们在这个级别上比模型慢。在其他情况下，这种上下文特异性是有帮助的：例如，在图8A所示的假设水平上，有一个工具可疑地完美地嵌入了一个洞。许多人很快就会注意到这一巧合，但由于该模型无法在不运行模拟的情况下评估工具如何适应环境，因此它只有10%的成功率。在未来的工作中，可以在SSUP框架中实例化更复杂的先验，但人们如何形成这些特定于上下文的先验，或者如何通过经验随着时间的推移塑造它们，这仍然是一个悬而未决的问题。

人们表现出比我们的模型更大的灵活性，能够从目标状态向后工作，找到更容易解决的子目标（24）。在图8B中的假设水平中，弹射器很挑剔，这意味着大多数弹射器动作都不会越过障碍物，因此永远不会击中左侧的球。相反，增加目标函数的最简单方法是采用不正确的策略，即向右击球以接近目标，因此该模型仅在8%的情况下解决了水平问题。向后工作以设定将球发射到障碍物上的第一个子目标，可以防止将球作为局部最小值而卡住。从工程的角度来看，在离散的问题空间中创建子目标是很自然的（12），但在虚拟工具游戏的连续动作空间中如何发现这些子目标尚不清楚。

Related Cognitive Systems:

对使用现实世界工具的认知系统进行了广泛的研究，包括了解如何操纵它们并了解它们的典型用途（例如，参考文献3、4、10和25）。在这里，我们的重点是工具的“机械知识”：如何在新的情况下使用物体。然而，在现实世界的工具使用中，这些系统与工具的运动规划和语义知识协同工作。未来的工作可以集中在这些联系上，例如新工具如何变得熟悉，或者我们的运动限制如何限制我们可能考虑的计划。

虚拟工具游戏提供了一个解决问题的任务，融合了先前工作的各个方面，但包含了一个新的挑战。要快速解决这些问题，需要对动力学有良好的先验知识，这与在不断变化的情况下学习动力学的复杂问题解决不同（26），一旦考虑到有前景的解决方案，还需要进一步迭代，这与洞察力问题解决中立即导致解决方案的“啊哈”时刻不同（27，28）。与传统的基于模型或无模型的强化学习不同，在这项任务中，人们带来了丰富的世界模型，可以快速针对特定的新问题进行定制

将丰富的世界知识提炼为有用的任务知识对于任何与复杂世界交互的代理都是必要的。关于如何实现这一点的一个建议是“通过思考学习”（29）：将知识从一个来源（物理的内部模型）翻译成另一个更具体的实例化（在这个特定层面上的行动和结果之间的映射）。我们展示了SSUP如何实例化一个通过思考学习的例子：通过使用来自内部模型的数据训练策略。这种知识转移的证据已经在人们身上发现（30,31），但主要集中在更简单的离散环境中，在这些环境中，模型和策略是共同学习的

Virtual Tools as an AI Challenge

在 model-free 强化学习方法的初步实验中（23），我们发现，尽管在相关级别上有丰富的经验，但在几乎所有虚拟工具级别（SI附录，S5节）上的泛化能力有限，学习效率低下

根据我们的人体实验，我们认为需要 model-based 的方法才能玩虚拟工具等游戏。这种方法在机器学习中越来越受欢迎（32），特别是当与可以学习快速适应新任务的“学习学习”技术相结合时（33，34）。学习这些模型仍然具有挑战性，但近年来，包含附加结构的方法表现出色（35，36）。在人工智能和机器人领域，基于模型的方法已经很受欢迎（9，37，38）。剩余的挑战包括如何学习足够准确的模型，以便与原始传感器数据一起使用（39），以及如何处理动态环境

虚拟工具增加了越来越多的环境，这些环境测试人工智能使用物理进行预测和推理的能力，例如同时开发的物理推理（PHYRE）基准（40）和其他（41-43）。相比之下，我们的重点是提供人们认为具有挑战性但直观的问题，解决方案是不显而易见的，也不依赖于对世界动态的精确了解。通过提供人类数据来比较人工智能和生物智能，我们希望为更多类似人类的人工智能提供一个试验台

Future Empirical Directions:

这项工作为灵活工具使用的计算和经验基础的形式化提供了初步尝试，但仍有许多需要研究的地方。例如，我们没有发现人们对世界了解得更多的证据，也许是因为在这里增加精确度几乎没有什么好处。但在某些情况下，学习动力学显然是有帮助的（例如，发现一个物体异常重或粘在一起），我们希望人们在这些情况下更新他们的身体信念。人们何时以及以何种方式更新其内部模型以支持规划是一个重要的研究领域。

孩子们可以比制作新工具更早地发现如何使用现有物品（10），这表明工具创作比工具使用更具挑战性。然而，理论上推动人类文化的是制造并传递新工具的能力（44）。因此，不仅要了解人们如何使用工具，还要了解他们如何开发和传播工具，我们可以通过扩展虚拟工具游戏的动作空间来研究这一点

Conclusion:

了解如何灵活地使用工具来实现我们的目标是一种基本和核心的认知能力。在虚拟工具游戏中，我们发现人们有效地使用工具来解决各种各样的物理问题。我们可以用SSUP框架的三个组成部分来解释这种快速试错学习：丰富的先验世界知识、假设动作的模拟以及从模拟和观察到的动作中学习的能力。我们希望这个经验领域和建模框架能够为未来研究这一典型的人类特征提供基础：使用、制造和推理工具，以及更广泛地塑造物理世界以达到我们的目的